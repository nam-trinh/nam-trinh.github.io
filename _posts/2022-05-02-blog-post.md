---
title: 'A friendly introduction to Transformers '
date: 2022-05-02
permalink: /posts/2022-05-02-blog-post
tags:
  - deep learning 
  - computer vision
  - natural language processing
  - transformers
---
{% include base_path %}
Post Status: In Progress

After the introduction of Transformer in 2018 in the infamous paper "Attention is all you need" [1] from Google Research team, a wide range of applications in natural language processing is now heavily based on Transformer. In this post, I will introduce the underlying mechanism of Transformer by attempting to answer three questions:
- Why do we need Transformer?
- How does Transformer work? 
- What are the applications of Transformer?

1. Why do we need Transformer?

Two key advantages of Transformer are: fast to train and fixing the exploding/vanishing gradient. I will explain these two advantages in detail by first looking at the disadvantages of recurrent neural networks (RNN) like Long Short Term Memory (LSTM) or Gated Recurrent Unit (GRU). 

RNN has several types of models: sequence-vector models (e.g. sentiment analysis), vector-sequence models (e.g. ), and sequence-sequence models. 

2. How does Transformer works?

3. What are the applications of Transformer?

References:
[1] Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30 (2017).
