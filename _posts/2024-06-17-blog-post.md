---
title: "An introduction to Transformers "
date: 2024-06-17
permalink: /posts/2024-06-17-blog-post
tags:
  - deep learning
  - computer vision
  - natural language processing
  - transformers
---

# Transformers Tutorial: "Attention is All You Need"

## Introduction

So, you've probably used ChatGPT for your own writing before. Ever wondered what's powering it? The magic behind ChatGPT is something called transformers (yes, just like the robot movies!). The Transformer model, introduced in the paper "Attention is All You Need" by Vaswani et al., has completely changed the game in natural language processing (NLP). Instead of using traditional recurrent or convolutional neural networks, it relies on a purely attention-based mechanism. This innovative approach is the backbone of many top-performing NLP models like BERT, GPT, and T5. Despite its significance, understanding the architecture of transformers and the reasoning behind their design isn't exactly straightforward. It took me months of watching countless YouTube tutorials and reading numerous papers before I finally got the hang of it! So I'm writing this tutorial and hopefully you can understand it intuitively. Due to the complexity of transformers, I will divide the tutorial into several blog posts.

## Key Concepts

### Attention Mechanism:

The core idea of the Transformer model is the attention mechanism, which allows the model to focus on different parts of the input sequence when producing each element of the output sequence. The key types of attention are:

- Self-Attention: Each word in a sequence attends to all other words in the same sequence to capture contextual relationships.

- Scaled Dot-Product Attention: Computes the attention scores using the dot product of query and key vectors, scaled by the square root of the dimension of the key vectors.

### Encoder-Decoder Architecture: The Transformer model uses an encoder-decoder structure:

- Encoder: Maps an input sequence to a continuous representation.

- Decoder: Uses the encoded representation to generate the output sequence.

Each of these parts consists of multiple layers of self-attention and feed-forward neural networks.

## Transformer Model Components

- Positional Encoding
  Since the Transformer model does not use recurrence or convolution, positional encodings are added to the input embeddings to retain the order of the sequence. These encodings are computed using sine and cosine functions of different frequencies.

- Multi-Head Attention
  Instead of a single attention function, the Transformer employs multiple attention heads to capture different aspects of the relationships between words. Each head performs attention independently, and their outputs are concatenated and linearly transformed.

- Layer Normalization and Residual Connections
  Each sub-layer (attention and feed-forward) in the encoder and decoder has a residual connection around it, followed by layer normalization. This helps in training deep networks by mitigating issues like vanishing gradients.

- Feed-Forward Networks
  Each position in the sequence is independently passed through a fully connected feed-forward network. This network consists of two linear transformations with a ReLU activation in between.

## Step-by-Step Walkthrough in Python

Let's get a hands-on on how you can code a transformers from scratch!

1. Input Embedding and Positional Encoding
   The input sequence is first transformed into embeddings, which are then added to positional encodings to incorporate positional information.

```python
def get_positional_encoding(max_len, d_model):
    position = np.arange(max_len)[:, np.newaxis]
    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
    pe = np.zeros((max_len, d_model))
    pe[:, 0::2] = np.sin(position * div_term)
    pe[:, 1::2] = np.cos(position * div_term)
    return pe

# Example usage
positional_encoding = get_positional_encoding(50, 512)

```

2. Scaled Dot-Product Attention

The attention mechanism computes a weighted sum of values, where the weights are determined by the query-key dot product.

```python
def scaled_dot_product_attention(Q, K, V, mask=None):
    d_k = Q.shape[-1]
    scores = np.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    attention_weights = softmax(scores, axis=-1)
    return np.matmul(attention_weights, V), attention_weights
```

3. Multi-Head Attention

Multi-head attention allows the model to focus on different parts of the sequence simultaneously.

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, num_heads, d_model):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0
        self.d_k = d_model // num_heads
        self.num_heads = num_heads
        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])
        self.output_linear = nn.Linear(d_model, d_model)

    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        query, key, value = [l(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) for l, x in zip(self.linear_layers, (query, key, value))]
        x, attention_weights = scaled_dot_product_attention(query, key, value, mask=mask)
        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)
        return self.output_linear(x)
```

4. Encoder Layer
   Each encoder layer consists of multi-head attention and feed-forward network sub-layers.

```python
class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super(EncoderLayer, self).__init__()
        self.self_attn = MultiHeadAttention(num_heads, d_model)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Linear(d_ff, d_model)
        )
        self.layer_norms = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(2)])
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        x = x + self.dropout(self.self_attn(self.layer_norms[0](x), x, x, mask=mask))
        x = x + self.dropout(self.feed_forward(self.layer_norms[1](x)))
        return x

```

5. Decoder Layer
   The decoder layer is similar to the encoder layer but includes an additional multi-head attention layer for attending to the encoder's output.

```python
class DecoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super(DecoderLayer, self).__init__()
        self.self_attn = MultiHeadAttention(num_heads, d_model)
        self.cross_attn = MultiHeadAttention(num_heads, d_model)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Linear(d_ff, d_model)
        )
        self.layer_norms = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(3)])
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, memory, src_mask=None, tgt_mask=None):
        x = x + self.dropout(self.self_attn(self.layer_norms[0](x), x, x, mask=tgt_mask))
        x = x + self.dropout(self.cross_attn(self.layer_norms[1](x), memory, memory, mask=src_mask))
        x = x + self.dropout(self.feed_forward(self.layer_norms[2](x)))
        return x
```

6. Full Encoder and Decoder
   The full encoder and decoder stack multiple encoder and decoder layers.

```python
class Encoder(nn.Module):
    def __init__(self, num_layers, d_model, num_heads, d_ff, input_vocab_size, max_len, dropout=0.1):
        super(Encoder, self).__init__()
        self.embedding = nn.Embedding(input_vocab_size, d_model)
        self.positional_encoding = get_positional_encoding(max_len, d_model)
        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        x = self.embedding(x) + self.positional_encoding[:x.size(1), :]
        x = self.dropout(x)
        for layer in self.layers:
            x = layer(x, mask)
        return x

class Decoder(nn.Module):
    def __init__(self, num_layers, d_model, num_heads, d_ff, target_vocab_size, max_len, dropout=0.1):
        super(Decoder, self).__init__()
        self.embedding = nn.Embedding(target_vocab_size, d_model)
        self.positional_encoding = get_positional_encoding(max_len, d_model)
        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, memory, src_mask=None, tgt_mask=None):
        x = self.embedding(x) + self.positional_encoding[:x.size(1), :]
        x = self.dropout(x)
        for layer in self.layers:
            x = layer(x, memory, src_mask, tgt_mask)
        return x
```

7. Final Transformer Model
   Finally, we can define the full Transformer model by combining the encoder and decoder with a linear layer for the final output.

```python
class Transformer(nn.Module):
    def __init__(self, encoder, decoder, src_pad_idx, tgt_pad_idx, d_model):
        super(Transformer, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_pad_idx = src_pad_idx
        self.tgt_pad_idx = tgt_pad_idx
        self.linear = nn.Linear(d_model, decoder.embedding.num_embeddings)

    def make_src_mask(self, src):
        return (src != self.src_pad_idx).unsqueeze(-2)

```
